{"remainingRequest":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/vue-loader/lib/index.js??vue-loader-options!/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/src/views/Tema1.vue?vue&type=script&lang=js","dependencies":[{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/src/views/Tema1.vue","mtime":1732499125042},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/babel-loader/lib/index.js","mtime":456789000000},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/cache-loader/dist/cjs.js","mtime":499162500000},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/vue-loader/lib/index.js","mtime":499162500000}],"contextDependencies":[],"result":[{"type":"Buffer","data":"base64:CmltcG9ydCBBY29yZGlvbkEgZnJvbSAnLi4vYm9vdHN0cmFwL0Fjb3JkaW9uQScKZXhwb3J0IGRlZmF1bHQgewogIG5hbWU6ICdUZW1hMScsCiAgY29tcG9uZW50czogeyBBY29yZGlvbkEgfSwKICBkYXRhOiAoKSA9PiAoewogICAgLy8gdmFyaWFibGVzIGRlIHZ1ZQogIH0pLAogIG1vdW50ZWQoKSB7CiAgICB0aGlzLiRuZXh0VGljaygoKSA9PiB7CiAgICAgIHRoaXMuJGFvc1JlZnJlc2goKQogICAgfSkKICB9LAogIHVwZGF0ZWQoKSB7CiAgICB0aGlzLiRhb3NSZWZyZXNoKCkKICB9LAp9Cg=="},{"version":3,"sources":["Tema1.vue"],"names":[],"mappings":";AA+NA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"Tema1.vue","sourceRoot":"src/views","sourcesContent":["<template lang=\"pug\">\n.curso-main-container.pb-3\n  BannerInterno\n  .container.tarjeta.tarjeta--blanca.p-4.p-md-5.mb-5\n    .titulo-principal.color-acento-contenido\n      .titulo-principal__numero\n        span 1\n      h1 Construcción de #[i datasets]\n      \n    .row.justify-content-center.mb-4         \n      .col-lg-9.my-lg-0.my-3\n        p La construcción de datasets es el primer paso para cualquier proyecto de aprendizaje automático. Un dataset bien diseñado y construido es la base sobre la cual se desarrollan modelos robustos y precisos. En este capítulo, se abordará la importancia de los requerimientos y el diseño del dataset, se discutirán diversas técnicas de recolección de datos, y se analizarán los procesos necesarios para garantizar la calidad de los datos obtenidos. Estos elementos son clave para maximizar el valor de los modelos de #[i machine learning], ya que cualquier error en esta fase puede tener consecuencias significativas en las etapas posteriores.\n        .bg1.brad.p-3.j1\n          p.mb-0 En el ámbito del #[i Machine learning], la construcción de datasets robustos y representativos es fundamental para el éxito de cualquier proyecto. Un dataset, en esencia, es una colección organizada de datos que sirve como materia prima para entrenar y evaluar modelos de Inteligencia Artificial. Este capítulo explorará los aspectos clave en la construcción de datasets, desde la definición de requerimientos hasta las técnicas de recolección y control de calidad, con el objetivo de proporcionar al lector las herramientas necesarias para crear bases sólidas para sus proyectos de #[i Machine learning]. \n      .col-lg-3.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/3.png')   \n\n    Separador \n    #t_1_1.titulo-segundo.color-acento-contenido\n      h2 1.1 Requerimientos y diseño \n\n    .row.justify-content-center.mb-5 \n      .col-lg-5.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/6.png')              \n      .col-lg-7.my-lg-0.my-3\n        p Antes de construir un dataset, es esencial establecer los requerimientos y diseñarlo adecuadamente. Esta fase consiste en definir cuál es el objetivo del dataset, qué tipo de información se necesita recolectar, y cuáles serán las características más relevantes. Para diseñar un buen dataset, se debe considerar el tipo de datos que se van a manejar: datos estructurados, semiestructurados, o no estructurados. Cada tipo tiene implicaciones diferentes en cuanto a su manipulación y almacenamiento.\n        .bg4.brad.p-3.j1\n          p.mb-0 Antes de iniciar la recolección de datos, es esencial definir con precisión los objetivos del proyecto de #[i Machine learning]. ¿Qué problema se busca resolver? ¿Qué tipo de modelo se utilizará? ¿Qué preguntas se intentarán responder con los datos? Las respuestas a estas interrogantes guiarán la definición de los requerimientos del dataset.\n\n    p Aspectos por considerar en esta etapa:\n\n\n    .row.justify-content-center.text-center.mb-5\n      .col-lg-3.my-lg-0.my-3\n        .bg5.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/7.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg6.px-3.py-1                        \n              h5.mb-0.text-center Identificación de variables        \n          p.mb-0 Determinar las variable relevantes para el problema, incluyendo variables predictoras y variables objetivo.\n      .col-lg-3.my-lg-0.my-3\n        .bg5.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/8.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg6.px-3.py-1                        \n              h5.mb-0.text-center Tipo de datos         \n          p.mb-0 Definir el tipo de datos que se recolectará (numérico, categórico, texto, imágenes, etc.) y su formato (estructurado, no estructurado o semiestructurado).  \n\n      .col-lg-3.my-lg-0.my-3\n        .bg5.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/9.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg6.px-3.py-1                         \n              h5.mb-0.text-center Tamaño del #[i dataset]         \n          p.mb-0 Estimar el tamaño de la muestra necesaria para obtener resultados significativos, considerando la complejidad del problema y el algoritmo de #[i Machine learning] a utilizar.\n      .col-lg-3.my-lg-0.my-3\n        .bg5.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/10.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg6.px-3.py-1                        \n              h5.mb-0.text-center Fuente de datos        \n          p.mb-0 Identificar las fuentes de donde se obtendrán los datos, ya sean bases de datos existentes, APIs, web #[i scraping], encuestas, sensores, etc.               \n\n\n    .bloque-texto-g.bloque-texto-g--inverso.color-secundario.p-3.p-sm-4.p-md-5\n      .bloque-texto-g__img(\n        :style=\"{'background-image': `url(${require('@/assets/curso/temas/11.png')})`}\"\n      )\n      .bloque-texto-g__texto.p-4\n        p.mb-0 Además, el diseño del dataset incluye determinar la fuente de los datos: ¿provendrán de archivos CSV, bases de datos relacionales, o APIs externas? Definir cómo se capturarán los datos ayuda a garantizar la consistencia y disponibilidad necesaria para el modelado. En esta fase también se debe planificar cómo se realizará la transformación de datos y si se necesitan datos adicionales para enriquecer el conjunto. La selección de estas estrategias impactará directamente en la calidad del dataset y, por ende, en el rendimiento del modelo.\n        br\n\n        |Un diseño bien definido del dataset asegurará la calidad de los datos y facilitará las etapas posteriores del proceso de #[i Machine learning]. \n    .row.bg7.align-items-center\n      .px-lg-5.px-4  \n        Separador \n        #t_1_2.titulo-segundo.color-acento-contenido\n          h2 1.2 Técnicas de recolección \n\n        p Una vez que se ha diseñado el dataset, se debe proceder a la recolección de datos. Existen varias técnicas de recolección que se utilizan dependiendo del contexto y del tipo de datos que se necesiten. Algunas de las más comunes incluyen:\n\n        .row.justify-content-center.mb-4 \n          .col-lg-3.my-3\n            img.img-a.img-t(src='@/assets/curso/temas/13.png', alt='')          \n          .col-lg-9.my-3\n            AcordionA(tipo=\"a\" clase-tarjeta=\"tarjeta bg8\")\n              div(titulo=\"Web <i>scraping</i>\")\n                p Utilizado para recolectar datos de sitios web de manera automatizada. Este enfoque es útil para la recolección de datos no estructurados.\n              div(titulo=\"Consultas a bases de datos\")\n                p Utilizar consultas SQL o APIs de bases de datos relacionales permite la recolección de datos estructurados. Esta técnica es eficiente para acceder a información ya organizada.  \n              div(titulo=\"Formularios de entrada\")\n                p La recolección de datos directamente desde usuarios mediante formularios es particularmente útil para obtener información específica y controlada.  \n              div(titulo=\"APIs\")\n                p Acceder a datos a través de interfaces de programación de aplicaciones (APIs) proporcionadas por diferentes plataformas y servicios. \n              div(titulo=\"Encuestas\")\n                p Diseñar y administrar cuestionarios para recopilar información directamente de los individuos.  \n              div(titulo=\"Sensores\")\n                p Utilizar dispositivos que capturan datos del entorno, como temperatura, humedad, ubicación, etc.  \n\n        p.mb-5 Cada una de estas técnicas tiene sus ventajas y desventajas, y la elección dependerá de los objetivos del proyecto y de los recursos disponibles. Para facilitar la comprensión de las distintas técnicas, se presenta la siguiente tabla que resume algunas de sus principales características:  \n        \n        .row.justify-content-center.mb-5\n          .col-lg-10\n            .titulo-sexto.color-acento-botones\n              h5 Tabla 1. \n              span Técnicas de recolección de datos\n\n            .tabla-a.color-acento-botones.text-center\n              table\n                caption Fuente: OIT, 2024.\n                thead\n                  tr\n                    th Técnica de Recolección\n                    th Tipo de Datos\n                    th Ventajas\n                    th Desventajas\n                    th Herramientas\n\n                tbody\n                  tr\n                    td Web Scraping\n                    td No estructurados\n                    td Gran cantidad de datos\n                    td Problemas legales y de calidad\n                    td Beautiful Soup, Scrapy, Selenium\n                  tr\n                    td Consultas a Bases de Datos\n                    td Estructurados\n                    td Rápido y consistente\n                    td Limitado por la estructura\n                    td SQL, R, Python\n                  tr\n                    td Formularios de Entrada\n                    td Estructurados\n                    td Datos específicos y controlados\n                    td Requiere participación activa\n                    td Google Forms, SurveyMonkey\n                  tr\n                    td APIs\n                    td Estructurados\n                    td Acceso eficiente a datos\n                    td Dependencia del proveedor\n                    td Librerías de cliente para APIs en R y Python\n                  tr\n                    td Sensores\n                    td Estructurados\n                    td Datos en tiempo real\n                    td Requiere infraestructura física\n                    td Dispositivos IoT, Arduino, Raspberry Pi                                                              \n    .row.justify-content-center\n      .col-lg-10\n        .row.justify-content-center.align-items-center.bg9.p-4.brad\n          .col-lg-auto\n            img.img-a.img-t(src='@/assets/curso/temas/14.png' alt='')\n          .col.pt-lg-0.pt-md-4\n            p.mb-0 Esta tabla muestra las distintas opciones de recolección disponibles, lo que permite elegir la más adecuada según las necesidades del proyecto.\n\n    Separador \n    #t_1_3.titulo-segundo.color-acento-contenido\n      h2 1.3 Control de calidad\n\n    .row.justify-content-center.mb-5\n      .col-lg-8.my-lg-0.my-3\n        p Una vez recolectados los datos, es fundamental realizar un control de calidad para asegurar que los mismos sean adecuados para el entrenamiento de modelos de #[i machine learning]. La calidad de los datos tiene un impacto directo en el rendimiento de los modelos, por lo que cualquier error o inconsistencias pueden resultar en predicciones erróneas.\n        .bg4.brad.p-3.j1\n          p.mb-0 El control de calidad implica la identificación y eliminación de datos faltantes, valores duplicados o fuera de rango, y errores tipográficos. Además, es importante verificar la consistencia de los datos, asegurándose de que las unidades de medida sean correctas y uniformes. Otro aspecto importante del control de calidad es la detección de outliers, ya que estos pueden afectar negativamente la capacidad del modelo para generalizar los patrones. \n      .col-lg-4.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/15.png')  \n    p Algunas tareas comunes en el control de calidad de datos incluyen: \n\n    .row.justify-content-center.text-center.mb-5\n      .col-lg-4.my-lg-0.my-3\n        .bg10.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/16.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg11.px-3.py-1                        \n              h5.mb-0.text-center Limpieza de datos       \n          p.mb-0 Eliminar o corregir datos erróneos, faltantes o duplicados.\n      .col-lg-4.my-lg-0.my-3\n        .bg10.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/17.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg11.px-3.py-1                        \n              h5.mb-0.text-center Validación de datos  \n          p.mb-0 Verificar que los datos cumplan con los criterios de validez definidos en la etapa de diseño.\n\n      .col-lg-4.my-lg-0.my-3\n        .bg10.brad1.p-4.h-100\n          img.img-t.img-a.mb-4(src='@/assets/curso/temas/18.png' alt='')\n          .row.justify-content-center.mb-3\n            .col-auto.bg11.px-3.py-1                         \n              h5.mb-0.text-center Transformación de datos   \n          p.mb-0 Convertir los datos a un formato adecuado para el análisis, como la codificación de variables categóricas o la normalización de datos numéricos.  \n\n    .row.justify-content-center.align-items-center.mb-5\n      .col-lg-auto\n        img.img-a.img-t(src='@/assets/curso/temas/19.png' alt='')\n      .col.pt-lg-0.pt-md-4\n        p.mb-0 Para la ejecución del control de calidad, se pueden utilizar herramientas y bibliotecas como Pandas en Python, que permiten realizar operaciones de limpieza y transformación de manera eficiente. Por ejemplo, la eliminación de datos duplicados se realiza con funciones como drop_duplicates(), mientras que los datos faltantes pueden ser gestionados con fillna() para imputar valores. A continuación, se presenta un ejemplo visual que ilustra cómo es el proceso de control de calidad en un dataset, destacando las etapas desde la detección hasta la corrección de los errores:             \n\n    .row.justify-content-center.mb-5 \n      .col-lg-10\n        .titulo-sexto.color-acento-botones\n          h5 Figura 1.\n          span Flujo de control de calidad de datos\n        \n        .bgfig.p-5.brad.mb-2\n          img.img-a.img-t(src='@/assets/curso/temas/20.svg' alt='La Figura 1 se denomina « Flujo de control de calidad de datos» y presenta las etapas secuenciales de recolección, validación, limpieza y corrección en un ciclo que lleva al mejoramiento de la calidad en los datos.') \n        figcaption Fuente: OIT, 2024. \n\n\n    .row.justify-content-center\n      .col-lg-4.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/21.png')              \n      .col-lg-8.my-lg-0.my-3\n        p La figura muestra un diagrama de flujo que empieza con la recolección de datos, seguido de pasos como validación, limpieza, corrección y, finalmente, aprobación de los datos para su uso en el modelado. Este proceso cíclico asegura que los datos finales sean óptimos para el desarrollo del modelo.\n        p El monitoreo continuo de la calidad de datos debe incluir tanto verificaciones automatizadas como revisiones manuales periódicas. Las métricas de calidad deben definirse claramente y monitorearse de manera consistente. Algunos ejemplos comunes incluyen la completitud de los datos, la consistencia entre diferentes fuentes, y la adherencia a los formatos esperados.\n        .bg4.brad.p-3.j1\n          p.mb-0 Al concluir este capítulo, se debe enfatizar que la construcción del dataset no es solo una tarea técnica, sino una fase crítica que determina el éxito de los modelos de #[i machine learning]. Un enfoque riguroso en el diseño, la recolección y el control de calidad de los datos resultará en un conjunto de datos que pueda ser confiablemente usado para entrenamiento y predicción, minimizando así los riesgos de error en etapas posteriores.\n\n</template>\n\n<script>\nimport AcordionA from '../bootstrap/AcordionA'\nexport default {\n  name: 'Tema1',\n  components: { AcordionA },\n  data: () => ({\n    // variables de vue\n  }),\n  mounted() {\n    this.$nextTick(() => {\n      this.$aosRefresh()\n    })\n  },\n  updated() {\n    this.$aosRefresh()\n  },\n}\n</script>\n\n<style lang=\"sass\"></style>\n"]}]}