{"remainingRequest":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/vue-loader/lib/index.js??vue-loader-options!/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/src/views/Tema4.vue?vue&type=script&lang=js","dependencies":[{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/src/views/Tema4.vue","mtime":1732499125042},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/babel-loader/lib/index.js","mtime":456789000000},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/cache-loader/dist/cjs.js","mtime":499162500000},{"path":"/home/runner/work/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/CF5_228136_PROCESAMIENTO_DATOS_MODELOS_IA/node_modules/vue-loader/lib/index.js","mtime":499162500000}],"contextDependencies":[],"result":[{"type":"Buffer","data":"base64:CmltcG9ydCBBY29yZGlvbkEgZnJvbSAnLi4vYm9vdHN0cmFwL0Fjb3JkaW9uQScKZXhwb3J0IGRlZmF1bHQgewogIG5hbWU6ICdUZW1hMycsCiAgY29tcG9uZW50czogeyBBY29yZGlvbkEgfSwKICBkYXRhOiAoKSA9PiAoewogICAgLy8gdmFyaWFibGVzIGRlIHZ1ZQogIH0pLAogIG1vdW50ZWQoKSB7CiAgICB0aGlzLiRuZXh0VGljaygoKSA9PiB7CiAgICAgIHRoaXMuJGFvc1JlZnJlc2goKQogICAgfSkKICB9LAogIHVwZGF0ZWQoKSB7CiAgICB0aGlzLiRhb3NSZWZyZXNoKCkKICB9LAp9Cg=="},{"version":3,"sources":["Tema4.vue"],"names":[],"mappings":";AA4HA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA;AACA","file":"Tema4.vue","sourceRoot":"src/views","sourcesContent":["<template lang=\"pug\">\n.curso-main-container.pb-3\n  BannerInterno\n  .container.tarjeta.tarjeta--blanca.p-4.p-md-5.mb-5\n    .titulo-principal.color-acento-contenido\n      .titulo-principal__numero\n        span 4\n      h1 Preparación para modelos \n\n    .bloque-texto-g.color-secundario.p-3.p-sm-4.p-md-5\n      .bloque-texto-g__img(\n        :style=\"{'background-image': `url(${require('@/assets/curso/temas/36.png')})`}\"\n      )\n      .bloque-texto-g__texto.p-4\n        p.mb-0 La fase final en el procesamiento de datos antes del entrenamiento de modelos de #[i machine learning] requiere una serie de transformaciones específicas que optimicen el rendimiento del algoritmo. Esta etapa resulta determinante para el éxito del modelo, pues los algoritmos de aprendizaje automático son sensibles a la escala y formato de los datos de entrada.      \n\n    Separador \n    #t_4_1.titulo-segundo.color-acento-contenido\n      h2 4.1 Escalamiento y normalización                      \n\n    .row.justify-content-center.mb-5\n      .col-lg-8.my-lg-0.my-3\n        .bg4.brad.p-3.j1.mb-4\n          p.mb-0 El escalamiento y normalización de datos constituyen transformaciones matemáticas que ajustan los valores numéricos a rangos específicos, facilitando el proceso de aprendizaje del modelo. Estas técnicas cobran especial relevancia cuando las variables presentan escalas muy diferentes entre sí.      \n        p La normalización min-max ajusta los valores a un rango específico, típicamente entre 0 y 1, preservando las relaciones entre los datos originales. Por otro lado, la estandarización (o normalización Z-score) transforma los datos para que tengan media cero y desviación estándar unitaria. La elección entre estas técnicas depende del algoritmo y la naturaleza de los datos. La siguiente tabla presenta una comparación detallada de las principales técnicas de escalamiento y sus casos de uso:\n\n      .col-lg-4.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/41.png') \n\n    .row.justify-content-center.mb-5\n      .col-lg-10\n        .titulo-sexto.color-acento-botones\n          h5 Tabla 5. \n          span Técnicas de escalamiento\n\n        .tabla-a.color-acento-botones.text-center\n          table\n            caption Fuente: OIT, 2024.\n            thead\n              tr\n                th Técnica\n                th Fórmula\n                th Rango resultante\n                th Casos de uso recomendados\n                th Consideraciones\n\n            tbody\n              tr\n                td Min-Max\n                td (x - min)/(max - min)\n                td [0,1]\n                td Redes neuronales, algoritmos basados en distancias\n                td Sensible a #[i outliers]\n              tr\n                td Z-Score\n                td (x - media)/desv.est\n                td [-∞,∞]\n                td Regresión lineal, SVM\n                td Asume distribución normal\n              tr\n                td Robust Scaler\n                td (x - mediana)/IQR\n                td Variable\n                td Datos con #[i outliers] significativos\n                td Más robusto a valores extremos\n              tr\n                td Log Transform \n                td log(x)\n                td [0,∞]\n                td Datos con distribución sesgada\n                td Solo para valores positivos                                                \n\n    Separador \n    #t_4_2.titulo-segundo.color-acento-contenido\n      h2 4.2 Codificación de variables\n\n    .row.justify-content-center.mb-5\n      .col-lg-4.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/42.png')              \n      .col-lg-8.my-lg-0.my-3\n        p La codificación de variables categóricas representa un paso esencial para convertir datos cualitativos en formatos numéricos que los algoritmos puedan procesar. Esta transformación debe realizarse cuidadosamente para preservar la información semántica contenida en las categorías originales.\n\n        .row.justify-content-center.align-items-center.bg9.brad1.p-4.mb-4\n          .col-lg-auto\n            img.img-a.img-t(src='@/assets/curso/temas/43.png' alt='')\n          .col.pt-lg-0.pt-md-4\n            p.mb-0 La codificación one-hot transforma cada categoría en una columna binaria, evitando la imposición de un orden artificial entre categorías. Sin embargo, puede generar matrices dispersas cuando el número de categorías es elevado. La codificación ordinal, por su parte, asigna números enteros a cada categoría y resulta más apropiada cuando existe un orden natural entre las categorías.\n\n        p En casos de variables categóricas con alta cardinalidad (muchas categorías únicas), técnicas más avanzadas como target encoding o feature hashing pueden ofrecer alternativas más eficientes. Estas técnicas deben aplicarse con precaución para evitar el sobreajuste, especialmente en conjuntos de datos pequeños.       \n\n    Separador \n    #t_4_3.titulo-segundo.color-acento-contenido\n      h2 4.3 Selección de características \n\n    .row.justify-content-center.mb-4         \n      .col-lg-8.my-lg-0.my-3\n        p La selección de características implica identificar el subconjunto más relevante de variables para el modelo, reduciendo la dimensionalidad del problema sin perder información significativa. Este proceso no solo mejora el rendimiento computacional sino que también puede aumentar la capacidad de generalización del modelo. \n        p Los métodos de selección de características se pueden clasificar en tres categorías principales:        \n      .col-lg-4.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/44.png')  \n\n    .row.justify-content-center.mb-4 \n      .col-lg-4.my-3\n        img.img-a.img-t(src='@/assets/curso/temas/40.png', alt='')                \n      .col-lg-8.my-3\n        AcordionA(tipo=\"a\" clase-tarjeta=\"tarjeta bg8\")\n          div(titulo=\"Métodos de filtro\")\n            p Evalúan las características de manera independiente del algoritmo de aprendizaje, utilizando métricas estadísticas como correlación o información mutua. Estos métodos son computacionalmente eficientes pero pueden pasar por alto interacciones complejas entre variables.\n          div(titulo=\"Métodos <i>wrapper</i>\")\n            p Utilizan el rendimiento del modelo como criterio de selección, evaluando diferentes subconjuntos de características. Aunque más precisos, resultan computacionalmente intensivos para datasets con muchas variables.\n          div(titulo=\"Métodos embebidos\")\n            p Realizan la selección de características como parte del proceso de entrenamiento del modelo, como ocurre con la regularización Lasso o los árboles de decisión.\n\n    .row.justify-content-center\n      .col-lg-7.my-lg-0.my-3\n        p Estas estrategias permiten asegurar una correcta división del #[i Dataset] y garantizar que los modelos entrenados tengan una buena capacidad para generalizar a datos no vistos, evitando problemas de sobreajuste o subajuste.\n        .bg9.brad.p-3.j1\n          p.mb-0 La validación cruzada juega un papel fundamental en la selección de características, ayudando a identificar aquellas que contribuyen consistentemente al rendimiento del modelo a través de diferentes subconjuntos de datos. Este proceso iterativo asegura que las características seleccionadas sean verdaderamente robustas y no dependen de particularidades de una partición específica de los datos.\n      .col-lg-5.my-lg-0.my-3.j1\n        img.img-t.img-a(src='@/assets/curso/temas/47.png')                           \n\n</template>\n\n<script>\nimport AcordionA from '../bootstrap/AcordionA'\nexport default {\n  name: 'Tema3',\n  components: { AcordionA },\n  data: () => ({\n    // variables de vue\n  }),\n  mounted() {\n    this.$nextTick(() => {\n      this.$aosRefresh()\n    })\n  },\n  updated() {\n    this.$aosRefresh()\n  },\n}\n</script>\n\n<style lang=\"sass\"></style>\n"]}]}